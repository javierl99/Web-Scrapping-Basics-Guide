{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmbPJfmD2Eu0tFtdCS3mfz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Web Scrapping\n","\n","Para tener un esquema general, siempre haremos los mismos pasos:\n","\n","\n","1.   Importar la página mediante get o page_source\n","2.   Transformar la web para ser legible con BeautifulSoup\n","3.   Buscar el elemento que nos interesa mediante find all() utilizando la clase o el tipo\n","4.   Extraer el texto de la sopa de letras mediante un .text\n","\n","Durante esta explicación se utilizarán métodos que fueron usados para un ecommerce y un exchange de cyptos respectivamente. En el primer caso nuestro objetivo es extraer el nombre del producto, su precio, imagen... Mientras que en el segundo caso buscaremos información sobre varias criptomonedas tras pulsar un botón\n","\n"],"metadata":{"id":"A1vZvW0InK1x"}},{"cell_type":"markdown","source":["# Instalación de Paquetes\n","\n","No tienes por que instalar todos, por ejemplo la priemra celda es obligatoria menos por  **image** que sirve para extraer imagenes de la web.\n","\n","De la sergunda celda podemos prescindir de image, nltk  y nltk download\n","\n","La tercera celda solo servirá para instalar selenium pero necesitará el BeautifuSoup y otros paquetes de arriba"],"metadata":{"id":"m0uSBTNwYpiq"}},{"cell_type":"code","source":["!pip3 install beautifulsoup4 --user  # Paquete para poder leer bien las paginas, en vez de una sopa de letras    \n","!pip3 install lxml --user            # Leector del lenguaje de las web   \n","!pip3 install html5lib --user        # Lenguaje html                 \n","!pip3 install image                  # Paquete para reconocer imagenes            "],"metadata":{"id":"hUPeFd_5Ynzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPMFpFPQYckv"},"outputs":[],"source":["import requests                    # Paquete para llamar a la pagina web, hay dos uno es este y el otro selenyum            \n","from bs4 import BeautifulSoup      # Importamos el paquete para leer la pagina                         \n","from PIL import Image              # Importamos el paquete para leer la imagen                      \n","from os.path  import basename                                \n","from collections import Counter                                \n","import pandas as pd                                \n","                                \n","import re                          # Paquete request      \n","import nltk                               \n","import image                       # Imagen  \n","\n","nltk.download(\"all\")"]},{"cell_type":"code","source":["!pip install selenium                                                      # Pip install for selenium                                                                               \n","!apt-get update                                                                 \n","!apt install chromium-chromedriver                                                  \n","\n","from selenium import webdriver                                             # Webdriver extrae la información de la web    \n","from selenium.webdriver.support.ui import WebDriverWait                    # Rapidez del bot a la hora de buscar la página y clicar                             \n","\n","\n","chrome_options = webdriver.ChromeOptions()                                 # Preparación del driver                \n","chrome_options.add_argument('--headless')                                                  \n","chrome_options.add_argument('--no-sandbox')                                                  \n","chrome_options.add_argument('--disable-dev-shm-usage')                                                  \n","driver =webdriver.Chrome('chromedriver',chrome_options=chrome_options)     # Driver ready                                            \n","wait = WebDriverWait(driver, 10)                                           # Velocidad del driver especificada"],"metadata":{"id":"x9NneyVYgr49"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importar y Transformar la web"],"metadata":{"id":"QYtiQHGTezN0"}},{"cell_type":"markdown","source":["Tenemos dos paquetes para hacer esto:\n","\n","*   Request\n","*   Selenyum\n","\n","**Veamos Request primero**\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"8Hq4aFAvfNMT"}},{"cell_type":"code","source":["response= requests.get(\"DESIRED_URL\") #Ruta de la pagina y obtención de información"],"metadata":{"id":"ZQFqsp83e-xl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["soup= BeautifulSoup(response.text, 'html.parser') #Transformamos la información a algo legible"],"metadata":{"id":"YvRuh-vjff_6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ahora veamos selenyum**\n","\n","A diferencia de  Request tendremos que primero decirle a nuestro driver qué página queremos y luego guardar sus resultados, para finalmente transformarlos"],"metadata":{"id":"-33XDk5CgRwg"}},{"cell_type":"code","source":["driver.get(\"DESIRED_URL\") #Este paso no era necesario con request, pero si con Selenyum"],"metadata":{"id":"k4PZaWjmiNWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response= driver.page_source             #Fijate como aquí usamos .page_source en vez de get, su función es la misma"],"metadata":{"id":"ewpIZqjjieVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["soup = BeautifulSoup(pagina_1, 'html.parser')   #Transformamos la información a algo legible"],"metadata":{"id":"mCBRwc5MixNr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Encontrar información\n","\n","Para obtener información sobre la web debemos abrir la web en una de nuestras pestañas y utilizar la herrmaienta Inspeccionar elemento. Aquí encontraremos todo el código HTML de la página web, de aquí debemos extraer el path, clase o referencia a los elementos de la página que nos interesan.\n","\n","Existen varios recursos para encontrar lo que buscamos en la web:\n"],"metadata":{"id":"ufiTyiQ8gSGy"}},{"cell_type":"markdown","source":["**Request**"],"metadata":{"id":"5FJ4V4Y1jUBP"}},{"cell_type":"code","source":["texto = soup.find_all(\"h3\")                          #Encontrar la información que tenga un titulo H1,H2,H3,H4.H5.H6\n","texto=soup.find_all(text=True)                       #Encontrar todo el texto de la página         \n","tabla = soup.find_all('table', class_= 'inbox')      #Encontraremos las tablas que sean clase inbox"],"metadata":{"id":"4Eb6HWWfjA59"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Selenyum**"],"metadata":{"id":"PVgzN8bhpXXw"}},{"cell_type":"code","source":["soup.findAll(class_ = \"a-section a-spacing-base\")    #Encontar mediante la clase usando Selenyum"],"metadata":{"id":"MN-YrtyzowKb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extraer el texto"],"metadata":{"id":"Z2rDdU6pp1vS"}},{"cell_type":"markdown","source":["**Selenyum**"],"metadata":{"id":"hB4Ia9q6qHpA"}},{"cell_type":"code","source":["soup.findAll(class_ = \"a-section a-spacing-base\")    #Encontar mediante la clase usando Selenyum"],"metadata":{"id":"dicUS69-qHpC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Request**"],"metadata":{"id":"bWiOBLcEqAR3"}},{"cell_type":"code","source":["for i in texto:                                      # Ya habremos obtenido todas las lineas de h3, con esta loop pasaremos por todo el codigo extrayendo solo el texto                                       \n","    print(i.text) \n","\n","\n","print(tabla.text)                                    #Las tablas no van por lieneas así que podemos hacer un.text directamente para extraer todo el texto"],"metadata":{"id":"6-Wrfftup-Bd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Selenyum**"],"metadata":{"id":"eLJkcaVlqFCw"}},{"cell_type":"code","source":["for elemento in resultado:\n","    titulo_producto = elemento.find(class_= \"a-size-base-plus a-color-base a-text-normal\").get_text()  #Utilizamos get.text en este caso\n","    imagen_producto = elemento.find(\"img\")                                                             #Tambien podemos realizar estas busquedas con selenyum"],"metadata":{"id":"TVYfIAeqqFCx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","--------------------------------------------------------------------------------------\n","\n"],"metadata":{"id":"WQgXysfoqyxf"}},{"cell_type":"markdown","source":["# Sección avanzada: Listado ICO\n","\n","Comentamos por comprobar cuantas tablas hay para luego extraer la que nos interesa, usando selenium"],"metadata":{"id":"QwsnCxuzqqyG"}},{"cell_type":"code","source":["#Vamos a ver por cuantas tables hay y que clases hay dentro de ellas, para así poder seleccionarlas\n","for table in soup.find_all('table'):\n","    print(table.get('class'))"],"metadata":{"id":"Rr389QxcqmDR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Solo tenemos una table en este caso pero debemos elegirla de todas forma con el siguiente comando:\n","tables = soup.find_all('table')\n","table = soup.find('table', class_='sc-e19573c7-2 gJWiZE cmc-table ''')"],"metadata":{"id":"Cam4O4wbrOcX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Crearemos una loop para buscar la información ya acotada, porque nuestra variable table es solo una table de la página web, para luego extraerla en un DataFrame.\n","\n","Debemos tener en cuenta el comienzo de la loop, las tablas en html tienen tr y td, que es lo mismo que table row y table data. Por tanto, vamos a iterar por cada table row para encontrar cada table data y extraer de ella sus columnas, que es donde está nuestra información"],"metadata":{"id":"f-Z2za9OruGE"}},{"cell_type":"code","source":["#Vamos a iterar nuestra tabla para extraer todas las rows y filtrar su información para nuestras columnas, finalmente la añadimos al DF\n","indice = ['name','symbol', 'ico_price','stage', 'start_date', 'end_date', 'goal', 'launchpad'] #Si es verdad que nos costaría escribir todos los indices si fueran 1k PERO podemos intentar hacerlo automatico 'facilmente'\n","df_ico = pd.DataFrame(columns= indice)\n","\n","for row in table.tbody.find_all('tr'):                    \n","  columnas = row.find_all('td')\n","\n","  if(columnas != []):\n","        #.find(\"span\").text                                               #Hay varios a poner, al final lo importante es saber qeu existe span, text y contents  \n","        name= columnas[0].span.text                                       #.span.text (https://stackoverflow.com/questions/21823229/finding-next-occurring-tag-and-its-enclosed-text-with-beautiful-soup)\n","        symbol = columnas[0].span.find_next_sibling('span').text          #Nos dará el siguiente texto entre el tag span\n","        ico_price= columnas[1].text.strip()\n","        stage= columnas[2].span.contents[0].strip('&0.')\n","        start_date=columnas[3].text.strip()\n","        end_date=columnas[4].text.strip()\n","        goal=columnas[5].text.strip()\n","        launchpad=columnas[6].text.strip()\n","        df_ico = df_ico.append({'name': name, 'symbol' : symbol, 'ico_price' : ico_price, 'stage' : stage, 'start_date' : start_date, 'end_date' : end_date, 'goal' : goal, 'launchpad' : launchpad}, ignore_index=True)\n"],"metadata":{"id":"K6XP_KQbrswj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pulsar un boton y seguir extrayendo data\n","\n","Se puede dar el caso en el que necesitemos pulsar un boton, aquí tenía que pasar de la página 1 a la 2, luego a la 3 y así sucesivamente. \n","\n","Para ello, debemos comunicarle al driver que volvemos a la página principal y asegurarnos que estamos en ella\n","\n","Para interactuar con la web de esta manera siempre deberemos usar Selenyum\n","\n"],"metadata":{"id":"x9R4Sk88tWoF"}},{"cell_type":"code","source":["driver.get('DESIRED_URL')"],"metadata":{"id":"71kaBg9RzfA3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(driver.current_url)"],"metadata":{"id":"dIeVR1ruyDl9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y ahora es cuando viene lo interesante, en mi caso tuve varios problemas pues cuando corría la loop me ponía que había un error cuando intentaba pulsar el boton. En este caso salía que había más de un elmento, incluso infnormaba de que era banner.\n","Entonces borre el cache y cookies de la página web para ver qué pasaba cuando selenyum abría la página. Ahí encontre el código HTML del cookie banner, entonces:\n","\n","\n","*   Cree un range con el número de páginas que me interesaba visitar, junto a una condición para que solo clicara en el banner en la primera página, ya que una vez cerrado no volvía  a salir en las páginas posteriores\n","*   Copie el path del banner, que es ***uno de los mejores metodos para obtener un elemento*** \n","*   Copie y configure el path del boton de número de página que quería cambiar, que coincide con nuestro range.\n","*   Inserte la misma loop que la página principal\n","\n"],"metadata":{"id":"FGOBIuINuC3t"}},{"cell_type":"code","source":["first= 3\n","for i in range (first,13):\n","  time.sleep(20)\n","  if i == first:\n","    driver.find_element('xpath','//*[@id=\"cmc-cookie-policy-banner\"]/div[2]').click()\n","    driver.find_element('xpath',f'//*[@id=\"__next\"]/div/div[1]/div[2]/div/div/div[3]/p/div/ul/li[{i}]').click() # Añadiendo f' al comienzo de nuestra string nos permitirá añadir variables a nuestra string\n","  else:\n","    driver.find_element('xpath',f'//*[@id=\"__next\"]/div/div[1]/div[2]/div/div/div[3]/p/div/ul/li[{i}]').click()\n","  print(driver.current_url)\n","  web_ico2 = driver.page_source\n","  soup2 = BeautifulSoup(web_ico2, 'html.parser')\n","  table2 = soup.find('table', class_='sc-e19573c7-2 gJWiZE cmc-table ''')\n","  for row in table2.tbody.find_all('tr'):\n","    columnas = row.find_all('td')\n","    if(columnas != []):\n","          #.find(\"span\").text\n","          name= columnas[0].span.text                                    #.span.text (https://stackoverflow.com/questions/21823229/finding-next-occurring-tag-and-its-enclosed-text-with-beautiful-soup)\n","          symbol = columnas[0].span.find_next_sibling('span').text          #Nos dará el siguiente texto entre el tag span\n","          ico_price= columnas[1].text.strip()\n","          stage= columnas[2].span.contents[0].strip('&0.')\n","          start_date=columnas[3].text.strip()\n","          end_date=columnas[4].text.strip()\n","          goal=columnas[5].text.strip()\n","          launchpad=columnas[6].text.strip()\n","          df_ico = df_ico.append({'name': name, 'symbol' : symbol, 'ico_price' : ico_price, 'stage' : stage, 'start_date' : start_date, 'end_date' : end_date, 'goal' : goal, 'launchpad' : launchpad}, ignore_index=True)\n","          print(df_ico.tail(), '\\n')\n","          print('next')\n"],"metadata":{"id":"tk7wB6LykAL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","df_ico.to_pickle( 'route/nombre_deseado.pkl')  #Guardemos toda nuestra data para no tener que volver a hacer scraping y poder trabajar cone lla"],"metadata":{"id":"4J6rfYT9q268"},"execution_count":null,"outputs":[]}]}